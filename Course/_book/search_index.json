[["index.html", "Régression linéaire Chapitre 1 Théorie 1.1 Variables dépendantes et variables indépendantes 1.2 Equation linéaire 1.3 Modélisation linéaire 1.4 Résidus 1.5 Régression linéaire : une approche intuitive 1.6 La régression linéaire mathématiquement 1.7 Notion de corrélation 1.8 \\(R^{2}\\), part de varianace expliquée et non expliquée", " Régression linéaire Baptiste CRINIERE-BOIZET 2023-10-16 Chapitre 1 Théorie 1.1 Variables dépendantes et variables indépendantes En modélisation linéaire, il existe une distinction importante entre les variables : les variables dépendantes et indépendantes. La dépendance d’une variable n’est pas vraiment une propriété d’une variable, mais le résultat du choix de l’analyste de données. Lorsque nous examinons la relation entre deux variables, décider si l’une est indépendante de l’autre repose sur la logique ou la théorie. Par exemple, en étudiant la taille d’une mère et de son enfant, il est logique de considérer la taille de l’enfant comme dépendante de celle de la mère, car les gènes sont transmis de la mère à l’enfant. Ainsi, la taille de l’enfant dépend en partie de celle de la mère, avec la taille de l’enfant comme variable dépendante et la taille de la mère comme variable indépendante. La variable dépendante est aussi nommée variable réponse ou à expliquer. Une variable indépendante est appelée prédicteur ou explicative. Par exemple, la taille d’un enfant peut être expliquée par les gènes hérités de sa mère, faisant de la taille de l’enfant la variable réponse et celle de la mère la variable explicative. On peut aussi prédire la taille adulte d’un enfant à partir de la taille de sa mère. Dans une modélisation linéaire visant à explorer la relation entre plusieurs variables, il ne peux exister qu’une unique variable dépendante (celle que l’on cherche à expliquer) et plusieurs variables indépendantes utilisées pour expliquer cette variable dépendante. On nomme la variable dépendante \\(Y\\) et les variables explicatives \\(X_{1}\\), \\(X_{2}\\), etc. Figure 1.1: Nuage de points 1.2 Equation linéaire Une relation linéaire entre deux variables peut être représentée par une équation sous la forme \\(Y=b+aX\\), où \\(Y\\) est la variable dépendante, \\(X\\) est la variable indépendante, \\(b\\) est l’ordonnée à l’origine, et \\(a\\) est le coefficient directeur de la droite. Graphiquement, cette relation est représentée par une droite. Par convention, on place \\(Y\\) sur l’axe vertical (ordonnées) et \\(X\\) sur l’axe horizontal (abscisses). Grâce à cette équation, en connaissant la valeur de \\(X\\), on peut déterminer celle de \\(Y\\), rendant ainsi la compréhension et l’analyse de la relation plus simples. Figure 1.2: Equation linéaire 1.3 Modélisation linéaire La régression linéaire est une technique mathématique utilisée pour modéliser la relation entre deux variables continues. Bien que les relations linéaires parfaites soient courantes en théorie, dans la pratique avec des données réelles, les choses sont rarement aussi nettes. En observant le nuage de points de la figure ci-dessous figure, on note une tendance linéaire, même si les données ne s’alignent pas parfaitement sur une droite, il y a du bruit. La régression linéaire vise donc à tracer cette ligne droite, ou ce modèle prédictif, qui approxime au mieux cette tendance observée. 1.4 Résidus Lors d’une régression linéaire, nous tentons d’ajuster une droite à un ensemble de données constitué de deux variables. Bien que cette droite puisse souvent approximer avec justesse les tendances observées, des imprécisions subsistent. Ces imprécisions sont les différences entre les valeurs réellement observées et celles prédites par notre droite d’ajustement. Ces différences, connues sous le nom de “résidus” ou “erreurs”, donnent une indication sur l’exactitude de notre modèle. Visuellement, lorsque nous observons des points situés au-dessus de la ligne de régression, cela signifie que la valeur observée est supérieure à celle prédite. Inversement, des points situés en dessous de la ligne indiquent une valeur observée inférieure à la prédiction. Mathématiquement, le résidu pour un point donné est calculé comme \\(e_i= y_i−\\hat{y}_i\\) , avec \\(y_i\\) est la valeur observée et \\(\\hat{y}_i\\) est la valeur prédite par le modèle. 1.5 Régression linéaire : une approche intuitive Pour déterminer la meilleure droite qui représente cette relation, nous avons besoin d’un critère de sélection. Une première idée serait d’assurer que, lorsqu’il y a une erreur dans notre prédiction, cette erreur est aussi souvent négative que positive. En termes plus techniques, cela signifie que la distribution de nos résidus (les différences entre les valeurs observées et celles prédites par notre modèle) doit être centrée sur 0. Cela garantit que l’erreur moyenne de nos prédictions est égale à 0. Toutefois, ce seul critère ne suffit pas pour garantir la qualité de notre modèle. Pour illustrer cette idée, considérons une droite horizontale qui représente la moyenne de la variable dépendante Y. Bien que l’erreur moyenne soit nulle pour cette droite, il est évident que ce modèle n’offre aucune capacité prédictive. En effet, il attribue simplement à Y sa valeur moyenne, sans prendre en compte la valeur de la variable indépendante X. Il est essentiel d’aller plus loin dans l’analyse pour s’assurer que notre droite est réellement la plus représentative de la relation entre nos deux variables. La régression linéaire cherche à positionner une droite au plus près des points de données. Idéalement, la somme des écarts entre cette droite et les points, appelés résidus, devrait être nulle, mais en réalité, on cherche à la minimiser, à la rendre le plus faible possible. Pour ce faire, on utilise le critère des moindres carrés, qui minimise la somme des résidus au carré, évitant ainsi les compensations entre résidus négatifs et positifs. Cette approche est directement liée à la notion de variance des résidus. La droite optimale aura donc une moyenne des résidus égale à zéro et une somme de leurs carrés (variance) la plus faible possible, représentant ainsi l’estimateur des moindres carrés. Voici trois droites de régression distinctes. La droite qui semble la mieux ajustée en minimisant les résidus est celle située tout à droite. 1.6 La régression linéaire mathématiquement La formulation vectorielle du modèle de régréssion linéaire est la suivante : \\(y = X\\beta + \\epsilon\\), avec \\(\\epsilon\\) qui représente les résidus (les erreurs par rapport à la modélisation) \\(\\epsilon \\sim N(0, \\sigma^2I_n)\\). On suppose en outre que chaque observation est indépendante. \\[ \\text{Avec : } y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\text{, } X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{np} \\end{bmatrix} \\text{, } \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\] L’estimateur des moindres carrés est donné par la formule suivante : \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\) Dans le cas d’une régression linéaire simple (avec une unique variable indépendante \\(x\\)) l’équation de la régression est la suivante \\(y_i = \\beta_0 + \\beta_1 x_i\\), les estimateurs des moindres carrés des coefficients sont donnés par : \\[ \\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\text{ et } \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\] où \\(\\bar{x}\\) et \\(\\bar{y}\\) sont les moyennes des observations \\(x\\) et \\(y\\), respectivement. 1.7 Notion de corrélation La corrélation est un outil statistique qui évalue la force et la direction de la relation linéaire entre deux variables numériques. Bien qu’une droite de régression des moindres carrés puisse être déterminée pour n’importe quel ensemble de deux variables numériques, sa pertinence dépend grandement des données en question. Dans l’exemple donné des deux droites de corrélation, la droite de régression linéaire sur le côté gauche de la figure semble bien ajustée aux données. En revanche, sur le côté droit, la droite ne correspond pas du tout aux données présentées. C’est Francis Galton qui a introduit l’idée de quantifier la capacité d’une droite de régression à prédire une variable dépendante. Karl Pearson a par la suite affiné cette notion, aboutissant au coefficient de corrélation de Pearson. Pour calculer ce coefficient, on standardise d’abord les deux variables en soustrayant leur moyenne et en divisant par leur écart-type. La formule est : \\[ r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \\] où \\(x_i\\) et \\(y_i\\) représentent les valeurs des variables, et \\(\\bar{x}\\) et \\(\\bar{y}\\) leurs moyennes respectives. Ce coefficient oscille entre -1 et 1. Un coefficient positif indique que lorsque l’une des variables augmente, l’autre fait de même. À l’inverse, un coefficient négatif signifie qu’une augmentation d’une variable est associée à une diminution de l’autre. En termes de force, la valeur absolue du coefficient de corrélation se rapprochant de 1 signifie une relation linéaire plus forte entre les deux variables. Un coefficient de 1 ou -1 indique une relation linéaire parfaite. Voici une représentation interactive qui, à l’aide de données simulées, vous permet d’ajuster la corrélation entre les variables x et y dans une plage allant de 0 à 1. 1.8 \\(R^{2}\\), part de varianace expliquée et non expliquée La variance expliquée fait référence à la partie de la variation totale de la variable dépendante (Y) qui est “capturée” ou “expliquée” par notre modèle de régression. En d’autres termes, c’est la quantité de variation de Y que nous pouvons prévoir à partir de X en utilisant notre modèle. Si cette variance est élevée, cela signifie que notre modèle est assez bon pour prédire la variable dépendante à partir de la (ou des) variable indépendante. La variance non-expliquée est la partie de la variation de la variable dépendante (Y) que notre modèle ne parvient pas à capturer ou à expliquer. C’est l’écart entre les valeurs réelles de Y et les valeurs prédites par notre modèle. Si cette variance est faible, cela signifie que notre modèle est précis. Si elle est élevée, cela peut indiquer que notre modèle ne capture pas certains éléments importants qui influencent Y. Le \\(R^2\\) est une mesure qui nous donne une idée de la proportion de la variance totale de \\(Y\\) qui est expliquée par le modèle. Il varie entre 0 et 1. Un R^2 proche de 1 signifie que notre modèle explique une grande partie de la variance de \\(Y\\), tandis qu’un \\(R^2\\) proche de 0 signifie que le modèle n’explique pas bien la variance de \\(Y\\). En termes simples, \\(R^2\\) est une mesure de la “qualité d’ajustement” de notre modèle à nos données. Le R² est donné par la formule suivante : \\[ R^2 = 1 - \\frac{\\text{Somme des carrés des résidus}}{\\text{Somme totale des carrés}} = 1 - \\frac{\\text{Variance non expliquée}}{\\text{Variance Totale}} = \\frac{\\text{Variance expliquée}}{\\text{Variance Totale}} \\] Pour mieux comprendre la notion de variance expliquée, prenons un exemple simple. Imaginons que nous voulons étudier la relation linéaire entre la taille et le poids des souris. La taille est notre variable indépendante, tandis que le poids est notre variable dépendante (à expliquer). D’abord, regardez la figure de gauche. Ici, nous traçons une droite représentant la moyenne du poids des souris, sans prendre en compte leur taille. Nous calculons ensuite la variance du poids. Cela nous donne une idée de la dispersion des poids des souris autour de cette moyenne. Ensuite, nous traçons une droite de régression linéaire qui montre la relation entre la taille et le poids des souris. Après cela, nous calculons la variance des résidus. C’est l’écart entre cette droite de régression et chaque point (souris) sur le graphique. En comparant les deux variances, nous remarquons que la variance des résidus est plus petite que la variance initiale du poids. Ce que cela nous indique, c’est qu’en prenant en compte la taille des souris, nous avons réduit l’incertitude ou la dispersion autour de la moyenne. C’est ce qu’on appelle la “variance expliquée”. Ainsi notre modèle explique \\(R² = 1 - \\frac{0.14}{0.32} = 56\\%\\) de la variance du poids des souris. "],["vérification-des-hypothèses.html", "Chapitre 2 Vérification des hypothèses 2.1 Introduction 2.2 Indépendance 2.3 Linéarité 2.4 Homoscédasticité 2.5 Valeurs extrêmes 2.6 Variable cachée", " Chapitre 2 Vérification des hypothèses 2.1 Introduction En statistiques, lorsqu’on utilise la régression linéaire pour modéliser la relation entre des variables, il est essentiel que certaines hypothèses soient respectées pour garantir la validité des résultats et des interprétations. Ces hypothèses sont au nombre de quatre : Linéarité : La relation entre les variables doit être linéaire. Indépendance : Les résidus, qui sont les écarts entre les valeurs observées et les valeurs prédites, doivent être indépendants les uns des autres. On parle aussi d’indépendace des sujets, des individus ou des observations. Homoscédasticité : Les résidus doivent présenter une variance constante à travers toutes les valeurs prédites. Normalité : La distribution des résidus doit suivre une distribution normale. Si ces conditions ne sont pas remplies, les conclusions tirées de la régression peuvent être incorrectes. Pour valider ces hypothèses, il est courant d’analyser les résidus ou de se questionner sur la nature de nos données. Pour mieux comprendre et illustrer ces notions, nous allons prendre un exemple concret : étudier la relation entre le poids et la taille. À travers cet exemple, nous verrons comment ces hypothèses jouent un rôle crucial dans l’interprétation des résultats. 2.2 Indépendance 2.2.1 Explications L’hypothèse d’indépendance, souvent décrite comme l’indépendance des observations ou des individus, est centrale en régression linéaire. Elle exige que les résidus, les écarts entre les valeurs observées et prédites, soient indépendants les uns des autres. L’importance de cette hypothèse réside dans sa capacité à assurer que le modèle ne néglige pas des facteurs clés. Par exemple, si des individus d’une même base de données sont regroupés en famille, leurs caractéristiques pourraient être influencées par des éléments génétiques et environnementaux. Ignorer de tels facteurs, comme l’appartenance familiale, peut fausser nos prédictions. La non-conformité à cette hypothèse peut biaiser les estimations des paramètres, compromettre la fiabilité des tests d’hypothèses et entraîner des conclusions erronées. De mauvais estimateurs peuvent, par exemple, suggérer des relations qui n’existent pas réellement ou masquer des relations existantes. Pour vérifier cette hypothèse, il est essentiel de visualiser la distribution des résidus. Une distribution non aléatoire des résidus ou des regroupements indique une possible violation. Au-delà de cette visualisation, il est crucial de comprendre l’origine des données et de s’interroger sur d’éventuelles variables omises. En bref, l’hypothèse d’indépendance assure que notre modèle est exhaustif et que les erreurs proviennent d’aléas, et non d’omissions ou de biais. 2.2.2 Exemples et figures : Nous avons une base de données recensant la taille de 60 adolescents en fonction de leur âge. Notre intention est de réaliser une régression linéaire entre ces deux variables. La taille est naturellement sélectionnée comme variable dépendante, et l’âge comme variable indépendante. Cependant, il y a une subtilité : nos données ne sont pas totalement indépendantes car ces adolescents sont répartis en trois familles distinctes, ce qui introduit une source de variabilité et rompt l’hypothèse d’indépendance. Sur le graphique suivant, la droite pointillée en rouge représente la régression sans considération de l’effet familial. Les trois autres droites colorées illustrent la relation linéaire spécifique à chaque famille. Il est notable que ces dernières présentent des pentes moins prononcées, suggérant qu’en omettant l’effet familial, on surestime la relation entre la taille et l’âge. Le second graphique illustre la différence de la relation entre les résidus et l’âge pour les deux modèles : celui sans effet familial à gauche, et celui prenant en compte cet effet à droite. Lorsque les résidus ne sont pas indépendants et que l’hypothèse d’indépendance n’est pas vérifiée, les résidus tendent à se regrouper par famille. Ce phénomène est corrigé lorsque l’effet familial est inclus dans le modèle. 2.3 Linéarité 2.3.1 Explications Cette hypothèse ne concerne pas la forme de la relation entre les variables en elles-mêmes, mais plutôt la forme de la relation entre les variables et les coefficients du modèle. En d’autres termes, même si les données semblent suivre une courbe ou une autre forme non linéaire, il est possible d’ajuster un modèle linéaire en transformant les données ou en ajoutant des termes supplémentaires au modèle, comme des termes quadratiques ou cubiques. Pour vérifier cette hypothèse, on vérifie si les résidus sont dispersés de manière aléatoire sans motif apparent ; si c’est le cas, c’est un bon signe. De plus, un simple graphique illustrant la relation entre nos variables peut offrir une première indication. Si les points semblent suivre une ligne droite, cela suggère une relation linéaire. Si l’hypothèse de linéarité n’est pas respectée, cela peut entraîner des estimations biaisées et des prédictions inexactes. Il est donc crucial de vérifier cette hypothèse avant de tirer des conclusions à partir d’un modèle de régression linéaire. 2.3.2 Exemple On essaie toujours d’évaluer la relation entre l’âge et la taille, avec un autre jeu de données. D’après les premières observations visuelles des deux graphiques ci-dessous, la relation entre ces deux variables ne suit pas un schéma linéaire, mais semble plutôt être représentée par une courbe concave. Notre premier modèle est le suivant : \\(Taille = \\beta_0 + Age \\times \\beta_1\\), il ne semble pas convenir. En examinant la courbe de résidus par rapport aux valeurs prédites (à droite sur la figure ci-dessous), on observe un lien entre la valeur des résidus et la valeur prédite. Cela indique une défaillance dans l’ajustement du modèle aux données actuelles. Pour améliorer cet ajustement, nous avons ensuite intégré un terme quadratique lié à l’âge, conduisant à un nouveau modèle : \\(Taille = \\beta_0 + Age \\times \\beta_1 + Age^{2} \\times \\beta_2\\) Le résultat est une courbe qui correspond beaucoup mieux à nos données. On constate aussi que les résidus n’ont plus une relation apparente avec les valeurs prédites, ce qui illustre une amélioration significative de l’adéquation du modèle. 2.4 Homoscédasticité 2.4.1 Explications L’hypothèse d’homoscédasticité, également connue sous le nom d’hypothèse des variances égales, stipule que la variabilité des erreurs (ou résidus) doit rester constante quelle que soit la valeur de la variable indépendante dans une régression linéaire. En d’autres termes, la dispersion des résidus ne devrait pas augmenter ou diminuer systématiquement avec la valeur prédite. Pour vérifier cette hypothèse, une méthode courante consiste à visualiser les résidus. En traçant les résidus par rapport aux valeurs prédites ou à la variable indépendante, la dispersion des points devrait être à peu près la même sur toute la plage de valeurs. Si, en revanche, les résidus forment un motif en forme d’entonnoir ou montrent une tendance croissante ou décroissante de la dispersion, cela pourrait indiquer une violation de l’homoscédasticité, appelée hétéroscédasticité. Si l’hétéroscédasticité est présente, cela peut affecter la fiabilité des estimations et des tests statistiques. Comme solution, plusieurs approches peuvent être adoptées. L’une des méthodes courantes consiste à transformer les données, par exemple en utilisant le logarithme de la variable dépendante. 2.4.2 Exemple Nous avons mené une nouvelle étude pour mesurer le temps de réaction en fonction de l’âge. Le temps de réaction est notre variable dépendante et l’âge est notre variable explicative. Après analyse, il semble y avoir un lien entre l’âge et le temps de réaction : plus une personne est âgée, plus son temps de réaction tend à être long. Toutefois, nous avons remarqué que plus l’âge augmente, plus les résultats varient, créant une sorte de dispersion autour de la droite de régression. Cette dispersion pourrait être due à une variabilité plus faible des temps de réaction chez les jeunes, tandis que chez les personnes plus âgées, les différences sont plus marquées. Cela nous amène à penser que l’hypothèse d’homoscédasticité n’est pas respectée ici. En d’autres termes, l’écart entre les valeurs réelles et les valeurs prévues par notre modèle tend à changer avec l’âge. Pour résoudre ce problème, on peut d’appliquer une transformation logarithmique sur les données du temps de réaction. En faisant cela, on observe que les différences par rapport à la droite de régression dépendent moins de l’âge. 2.5 Valeurs extrêmes 2.5.1 Explications Les valeurs extrêmes, ou “outliers”, peuvent perturber un modèle de régression linéaire en faussant la pente et l’ordonnée à l’origine. Elles augmentent également l’erreur résiduelle, diminuant la fiabilité du modèle. Pour les détecter, on utilise souvent des graphiques de résidus, où un résidu éloigné des autres signale un outlier. Face à ces valeurs, on peut envisager de les supprimer, effectuer des transformations de données. Il est crucial de comprendre leur origine, car elles peuvent révéler des informations pertinentes sur l’étude en cours. 2.5.2 Exemple Dans notre exemple actuel, il y a deux valeurs qui se démarquent nettement, indiquées par une couleur différente. Il est essentiel de réfléchir soigneusement avant de décider si nous devrions les garder ou les retirer de notre analyse. 2.6 Variable cachée 2.6.1 Explications L’impact d’une variable cachée est double. D’une part, elle peut fausser la relation apparente entre les variables indépendantes et dépendantes. D’autre part, elle peut masquer une relation réelle ou exagérer une relation inexistante. Pour savoir si une variable cachée pourrait être un problème, il faut se demander si une variable pertinente a été omise, ou si des résidus présentent des tendances inexplicables. Une autre indication est lorsque des variables, qui devraient théoriquement être non liées à la variable dépendante, montrent une corrélation significative. Les conséquences d’ignorer une variable cachée sont sérieuses. Elles peuvent mener à des conclusions erronées, influencer les décisions basées sur ces conclusions et rendre le modèle non généralisable. Pour résoudre ce problème, il est essentiel d’incorporer toutes les variables pertinentes dans le modèle, même si cela nécessite une collecte de données supplémentaire. "],["interprétation.html", "Chapitre 3 Interprétation 3.1 Régression simple 3.2 Régression multiple", " Chapitre 3 Interprétation 3.1 Régression simple On appelle régression linéaire simple une régression qui fait appel à une seule variable explicative. Cette variable peut être soit continue, soit catégorielle. Dans ce qui suit, nous illustrerons ces concepts à travers un exemple simple, en examinant le lien entre soit le poids et la taille, soit le poids et le sexe, pour un échantillon d’une centaine d’individus. 3.1.1 Prédicteur continue 3.1.1.1 Le modèle TTout d’abord, nous devons déterminer quelle sera la variable dépendante (celle à expliquer) et quelle sera la variable indépendante (celle qui explique). Dans le cas de la relation entre la taille et le poids, nous choisirons le poids comme variable dépendante à expliquer en fonction de la taille, qui sera la variable indépendante. Nous faisons ce choix parce que la taille est généralement un facteur stable, tandis que le poids peut varier davantage en fonction de divers facteurs environnementaux. L’équation du modèle est la suivante : \\(Poids = \\beta_{0} + \\beta_{1} \\times Taille + \\epsilon\\) À première vue, lorsque nous observons nos données, il semble y avoir un lien linéaire entre les deux variables. L’objectif de la section suivante sera de mesurer la force de ce lien. 3.1.1.2 Les coefficients Lorsque l’on excute le modèle linéaire en R, on obtient la sortie suivante, avec encadré en rouge les coefficients du modèle. On retrouve \\(\\beta_{0}\\) qui vaut -63 et \\(\\beta_1\\) qui vaut 0.81. Le modèle est donc le suivant : \\(Poids = -63 + 0.81 \\times Taille + \\epsilon\\). Pour interpréter les coefficients du modèle, nous commençons par \\(\\beta_0\\), qui nous indique que lorsque la taille est nulle, le poids serait de -63 kg, une information qui n’est pas très pertinente dans ce contexte. En revanche, le coefficient \\(\\beta_1\\) apporte plus d’informations : il spécifie qu’une croissance de la taille de \\(n\\) centimètres provoque en moyenne une hausse du poids de \\(n \\times 0.81\\) kg. Pour illustrer, une élévation de 10 cm entraîne une augmentation moyenne du poids de 8.1 kg. Le graphique ci-dessous présente l’augmentation moyenne du poids associée à une hausse de 10 cm de la taille avec notre modèle linéaire. 3.1.1.3 Test statistique Pour déterminer si une variable est statistiquement associée à notre prédicteur, nous procédons à un test statistique. Au cours de ce test, nous vérifions la validité de l’hypothèse suivante : \\(\\beta_1 = 0\\) (connue sous le nom d’hypothèse nulle), en opposition à l’hypothèse alternative \\(\\beta_1 \\ne 0\\). L’objectif est de voir si nous pouvons rejeter l’hypothèse selon laquelle \\(\\beta_1 = 0\\), car si \\(\\beta_1\\) est égal à zéro, alors il n’y a pas de lien entre nos deux variables, Poids et Taille. Lorsque nous réalisons un test, nous pouvons obtenir une p-valeur associée à ce test et la comparer au seuil de 0.05. Si la p-valeur est inférieure à ce seuil, nous rejetons l’hypothèse nulle. Dans notre cas, comme la p-valeur est très faible, nous pouvons rejeter l’hypothèse nulle et conclure à une relation statistiquement significative entre nos deux variables. 3.1.1.4 \\(R^{2}\\) Enfin, nous pouvons examiner le \\(R^{2}\\) pour déterminer la proportion de la variance du poids qui est expliquée par la taille. Nous trouvons qu’environ 46% de la variance du poids est expliquée par l’effet de la taille. Globalement, notre modèle linéaire a donc un bon pouvoir explicatif. 3.1.2 Prédicteur catégoriel Sur la représentation des deux distributions des poids en fonction du sexe, on remarque une différence significative : en moyenne, les hommes semblent plus lourds que les femmes. Lorsque nous modélisons le lien entre une variable continue dépendante et une variable discrète indépendante, nous devons choisir une modalité de référence pour la variable discrète. Dans ce cas, nous choisirons la modalité ‘Femme’ comme référence. Ainsi, dans la modélisation, le poids des hommes sera calculé comme une différence par rapport au poids des femmes. L’équation du modèle est la suivante : \\(Poids = \\beta_0 + \\beta_1 \\times \\text{1}_{\\{Homme\\}}\\), avec \\(\\text{1}_{\\{Homme\\}}\\) qui est une fonction indicatrice qui prend la valeur 1 si c’est un homme et 0 si c’est une femme. 3.1.2.1 Les coefficients Dans notre modèle, \\(\\beta_0\\) est utilisé pour symboliser le poids moyen observé chez les femmes, ici il est de 67.9kg. De l’autre côté, la somme \\(\\beta_0 + \\beta_1\\) donne une représentation du poids moyen chez les hommes, ici il est de 83.1kg. En d’autres termes, \\(\\beta_1\\) caractérise l’écart de poids moyen estimé entre les deux groupes (hommes et femmes), ici il est de 15.2kg. 3.1.2.2 Tests statistiques Pour déterminer s’il existe un lien statistiquement significatif entre le sexe et le poids, on vérifie si le coefficient \\(beta_1\\) associé au sexe dans notre modèle statistique est non-nul. La démarche est similaire à celle faite précédemment dans le cas continue. Dans des cas où la variable analysée présente plus de deux modalités, un test global est utilisé pour tester la non-nullité de tous les coefficients correspondants à cette variable, où le nombre de coefficients (d’indicatrices) est égal à n-1 (n étant le nombre de modalités). Dans notre cas, la p-valeur, indiquée en rouge, est inférieure au seuil de significativité de 0.05. Cela nous permet d’affirmer que le sexe est significativement associé au poids. 3.1.2.3 \\(R^{2}\\) Enfin, en examinant le \\(R²\\), nous pouvons déterminer la proportion de la variance du poids qui est expliquée par la taille. Dans notre cas, le sexe explique approximativement 43% de la variance du poids. 3.2 Régression multiple Désormais, on procède à l’analyse de l’effet conjoint des deux variables explicatives en question. La présence de plusieurs variables explicatives dans notre modèle fait référence à une régression multiple. 3.2.1 Ajustement Lorsque l’on ajoute plusieurs variables explicatives à un modèle, chaque coefficient associé à une variable explicative indique l’effet de cette variable sur la variable réponse, en “ajustant” ou en “tenant compte” des autres variables. Cela signifie qu’il montre l’effet unique de cette variable, tout en contrôlant les effets des autres variables explicatives. 3.2.1.1 Dans le cas de discret On parle d’ajustement sur une variable quand on examine la relation entre deux variables explicatives en les additionnant, créant ainsi une somme. Cela implique l’ajout de nos deux variables explicatives. Dans notre contexte, cette démarche est représentée par l’équation qui formalise notre modèle : \\(Poids = \\beta_0 + \\beta_1 \\times \\text{1}_{\\{Homme\\}} + \\beta_2 \\times Taille + \\epsilon\\) Dans une situation où l’on ajuste une variable continue et une variable discrète, le processus consiste à déterminer une ordonnée à l’origine distincte pour chaque catégorie de la variable discrète. Graphiquement, cela donne lieu à des droites parallèles, chacune représentant un sous-groupe d’une catégorie particulière. Il est à noter que toutes ces lignes ont la même pente, ce qui implique que la distance entre elles reste constante. Ainsi, lorsqu’on procède à un ajustement simple, nous présupposons que la relation entre la taille et le poids ne varie pas en fonction du sexe ; la seule distinction résidant dans un décalage constant entre les deux groupes. Les résultats obtenus à partir du modèle exécuté dans R indiquent que la relation entre la taille et le poids est représentée par le coefficient \\(\\beta_2 = 0.5\\). Cela signifie qu’une augmentation de 10 cm dans la taille est associée, en moyenne, à une augmentation de 5 kg du poids, et ce pour les individus des deux sexes. De plus, notre modèle suppose que l’écart de poids entre les hommes et les femmes est constant, indépendamment de la taille, et que cet écart est de \\(\\beta_1 =\\) 8.9 kg. Notre modèle explique environ 55% de la variance du poids. 3.2.1.2 Dans le cas continue Lorsque l’on considère deux variables continues pour expliquer une variable réponse, l’ajustement permet d’évaluer l’impact d’une variable tout en tenant compte de l’influence de l’autre. Cela signifie que si l’on augmente d’une unité l’une des variables explicatives, tout en maintenant l’autre constante, le changement associé dans la variable réponse est essentiellement attribué à cette unité supplémentaire de la première variable, avec la seconde variable étant fixée. Prenons un exemple. Supposons qu’on cherche à comprendre comment la taille influence le poids, tout en considérant un autre facteur continu : l’âge. L’âge pourrait moduler la relation entre taille et poids. Ainsi, deux individus de même taille pourraient avoir des poids différents en fonction de leur âge, à cause des changements physiologiques liés à l’âge, comme une diminution de l’activité physique ou le processus naturel de vieillissement. Le modèle s’écrit de la manière suivante : \\(Poids = \\beta_0 + \\beta_1 \\times Taille + \\beta_2 \\times Age + \\epsilon\\) Lorsque l’on illustre graphiquement la relation entre le poids (y) et la taille (x), tout en ajustant sur l’âge, cela donne lieu à une série de droites de régression. Chacune de ces droites correspond à une unité d’âge spécifique. En ajustant sur l’âge, nous supposons que l’effet de l’âge sur la relation entre la taille et le poids est constant, ce qui signifie que toutes ces droites de régression seront parallèles entre elles. Ainsi, chaque droite représente comment le poids varie avec la taille pour une tranche d’âge donnée. Lors de l’analyse de la sortie R de notre modèle, nous remarquons que la p-valeur associée à l’effet de la variable “âge” n’est pas inférieure au seuil de 0.05. Par conséquent, il semble que l’âge ne soit pas un facteur significatif pour la modélisation du poids. 3.2.2 Interaction Une interaction en régression se produit lorsque l’effet d’une variable sur la variable réponse dépend du niveau d’une autre variable. En d’autres termes, les variables interagissent entre elles pour influencer conjointement la variable réponse. Reprenons notre exemple sur l’effet de la taille et du sexe sur le poids. Nous avons déjà évoqué qu’à taille égale, les hommes peuvent en moyenne peser plus que les femmes. Cependant, il se pourrait aussi que la relation entre taille et poids ne soit pas la même selon le sexe. Autrement dit, l’augmentation de poids relative à chaque centimètre supplémentaire pourrait être plus prononcée chez les hommes que chez les femmes. Plusieurs facteurs pourraient expliquer cette différence relative : des différences métaboliques ou des habitudes liées au style de vie spécifiques à chaque sexe. Avec notre exemple le modèle s’écrit de la manière suivante : \\(Poids = \\beta_0 + \\beta_1 \\times \\text{1}_{\\{Homme\\}} + \\beta_2 \\times Taille + \\beta_3 \\times \\text{1}_{\\{Homme\\}} \\times Taille\\) On observe sur la figure que les hommes ont une pente plus forte que celle des femmes ce qui se traduit pour une augmentation de taille égale entre un homme et une femme, les hommes auront une augmentation de poids supérieure. "],["exemple-1-1.html", "Chapitre 4 Exemple 1 4.1 Importation et data management 4.2 Exploration des données 4.3 Analyse bi-variée relation entre nos covariables 4.4 Figure finale 4.5 Modélisation 4.6 Figure finale 4.7 Vérifications des hypothèses", " Chapitre 4 Exemple 1 Lien entre l’updrs on et la durée de la maladie, la mutation, sexe et l’âge at onset. UPDRS ~ Disease duration * Mutation + Sexe + Age at onset 4.1 Importation et data management ## Mutation Age_at_onset Sex Disease_duration Age UPDRS ## 1 m/m 40 Female 22 62 29.33609 ## 2 m/m 35 Male 19 54 35.22323 ## 3 m/m 34 Female 25 59 36.21443 ## 4 m/m 36 Male 24 60 45.43737 ## 5 m/m 47 Female 33 80 38.78300 ## 6 m/m 40 Male 32 72 39.65993 ## &#39;data.frame&#39;: 600 obs. of 6 variables: ## $ Mutation : chr &quot;m/m&quot; &quot;m/m&quot; &quot;m/m&quot; &quot;m/m&quot; ... ## $ Age_at_onset : int 40 35 34 36 47 40 43 49 42 34 ... ## $ Sex : chr &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; ... ## $ Disease_duration: int 22 19 25 24 33 32 6 29 39 14 ... ## $ Age : int 62 54 59 60 80 72 49 78 81 48 ... ## $ UPDRS : num 29.3 35.2 36.2 45.4 38.8 ... 4.2 Exploration des données Code data %&gt;% dplyr::select(-UPDRS) %&gt;% furniture::table1(Mutation, &quot;Age at onset at onset&quot; = Age_at_onset, &quot;Disease duration&quot; = Disease_duration, Age, splitby =~ Sex, test = TRUE, output = &quot;html&quot;) Female Male P-Value n = 288 n = 312 Mutation 0.143 f/f 54 (18.8%) 46 (14.7%) f/s 43 (14.9%) 57 (18.3%) m/f 55 (19.1%) 45 (14.4%) m/m 52 (18.1%) 48 (15.4%) m/s 40 (13.9%) 60 (19.2%) s/s 44 (15.3%) 56 (17.9%) Age at onset at onset 0.01 32.3 (6.0) 31.1 (5.6) Disease duration 0.62 20.3 (11.2) 19.8 (11.4) Age 0.114 52.6 (12.7) 50.9 (13.2) 4.3 Analyse bi-variée relation entre nos covariables 4.3.1 Age at onset &amp; Mutation Code aov &lt;- data %&gt;% anova_test(Age_at_onset ~ Mutation) data %&gt;% ggplot(aes(x = Mutation, y = Age_at_onset, color = Mutation))+ stat_boxplot(geom =&#39;errorbar&#39;, width = 0.4, lwd = 0.85)+ geom_boxplot(outlier.shape = NA, lwd = 0.85)+ geom_jitter(alpha = 0.45, shape = 1, size = 1)+ stat_summary(aes(fill = Mutation), fun = mean, geom = &quot;point&quot;, shape = 23, size = 3, position = position_dodge(width=0.75)) + theme_classic()+ scale_color_aaas()+ scale_fill_aaas()+ labs(subtitle = get_test_label(aov, detailed = TRUE), x = &quot;&quot;, y = &quot;Age at onset at onset (in years)&quot;)+ theme(legend.position = &quot;none&quot;, axis.line = element_line(size = 1, color = &quot;black&quot;), axis.text = element_text(face = &quot;bold&quot;, size = 11), axis.title = element_text(face = &quot;bold&quot;, size = 13)) Code ## Modélisation model1 &lt;- lm(Age_at_onset ~ Mutation, data = data) car::Anova(model1) ## Anova Table (Type II tests) ## ## Response: Age_at_onset ## Sum Sq Df F value Pr(&gt;F) ## Mutation 5320.6 5 42.732 &lt; 2.2e-16 *** ## Residuals 14791.8 594 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3.2 Lien entre Disease duration &amp; Mutation 4.3.3 Age &amp; Mutation 4.3.4 Age at onset &amp; Disease duration 4.3.5 Age &amp; Age at onset 4.3.6 Age &amp; Disease duration 4.4 Figure finale 4.5 Modélisation ## Anova Table (Type II tests) ## ## Response: UPDRS ## Sum Sq Df F value Pr(&gt;F) ## Disease_duration 5247.8 1 240.0291 &lt; 2.2e-16 *** ## Mutation_bis 3006.9 1 137.5313 &lt; 2.2e-16 *** ## Age 1123.2 1 51.3751 2.273e-12 *** ## Sex 32.4 1 1.4802 0.2242 ## Disease_duration:Mutation_bis 1250.5 1 57.1966 1.506e-13 *** ## Residuals 12986.8 594 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Disease_duration Mutation_bis ## 7.576537 4.406205 ## Age Sex ## 6.561542 1.020252 ## Disease_duration:Mutation_bis ## 5.136705 4.6 Figure finale 4.7 Vérifications des hypothèses 4.7.1 Résidus 4.7.2 Colinéarité ## Disease_duration Mutation_bis ## 7.576537 4.406205 ## Age Sex ## 6.561542 1.020252 ## Disease_duration:Mutation_bis ## 5.136705 "],["exemple-2-1.html", "Chapitre 5 Exemple 2", " Chapitre 5 Exemple 2 Associations between Sleep Duration, Sleep Quality, and Cognitive Test Performance among Older Adults https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4031401/ Age, neuropathology, and dementia https://pubmed.ncbi.nlm.nih.gov/19474427/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
